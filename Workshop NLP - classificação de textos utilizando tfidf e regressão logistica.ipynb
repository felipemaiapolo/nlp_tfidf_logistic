{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihWkG6fa8xwM"
   },
   "source": [
    "# Workshop NLP: classificação de textos utilizando tf-idf e regressão logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4JsDYPNQ3ZS"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SIvT7ih8xwV"
   },
   "source": [
    "Importando bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "CetHWu0AQyw7",
    "outputId": "854a0e1d-8a69-4a37-c95e-2c05205f0e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy \n",
    "import re\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDQCbZgNRUl2"
   },
   "source": [
    "Conectando ao Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "JlLJ5pWK8xwO",
    "outputId": "028b10eb-3344-4b06-aec5-76a1932c6469"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kS0ZGFuWRi_n"
   },
   "source": [
    "Definindo caminho dos arquivos (base de dados etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBvovfAW9iTO"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/Workshop2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXrjd7IwRqlt"
   },
   "source": [
    "## Começo dos trabalhos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIST-IKM8xwe"
   },
   "source": [
    "Lendo base de dados classificação de notícias (a base de dados pode ser baixada em https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "VKMJzw7M8xwf",
    "outputId": "4f3a0ad8-532b-41e3-dabd-da5bf9ee761b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>SciTech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     label\n",
       "0  Unions representing workers at Turner   Newall...  Business\n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...   SciTech\n",
       "2  AP - A company founded by a chemistry research...   SciTech\n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...   SciTech\n",
       "4  AP - Southern California's smog-fighting agenc...   SciTech"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carregando JSON \n",
    "\n",
    "data = []\n",
    "\n",
    "for line in open(path+'News Classification DataSet.json', 'r'):\n",
    "    data.append(json.loads(line))\n",
    "\n",
    "content, label = [], []\n",
    "for each in data:\n",
    "    content.append(each['content'])\n",
    "    label.append(each['annotation']['label'][0])\n",
    "    \n",
    "df = pd.DataFrame([content, label]).T\n",
    "df.columns= ['content', 'label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "592612YP8xwp"
   },
   "source": [
    "Vamos ver o shape da base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "68XR7UMW8xwr",
    "outputId": "aa56fab3-5e29-41c5-9e78-8e181bf29787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KECREDBT8xw3"
   },
   "source": [
    "Checando frequências relativas de labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "kvGTlWXj8xw5",
    "outputId": "71fb45a0-c004-4aed-ccd4-664c9bde10ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>freq_rel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SciTech</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     freq_rel\n",
       "label             \n",
       "Business      0.25\n",
       "SciTech       0.25\n",
       "Sports        0.25\n",
       "World         0.25"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=df['label'], columns=\"freq_rel\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQyZLpgl8xxB"
   },
   "source": [
    "Vamos transformar os 4 labels em 4 variáveis binárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "251bxzLQ8xxC",
    "outputId": "002e65fc-5781-4a5a-8943-bb3ae2eb4be7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>Business</th>\n",
       "      <th>SciTech</th>\n",
       "      <th>Sports</th>\n",
       "      <th>World</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>SciTech</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content     label  ...  Sports  World\n",
       "0  Unions representing workers at Turner   Newall...  Business  ...       0      0\n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...   SciTech  ...       0      0\n",
       "2  AP - A company founded by a chemistry research...   SciTech  ...       0      0\n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...   SciTech  ...       0      0\n",
       "4  AP - Southern California's smog-fighting agenc...   SciTech  ...       0      0\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([df,pd.get_dummies(df['label'])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bZq_4Lt8xxI"
   },
   "source": [
    "Vamos transformar nossa base de dados em duas listas (uma para os textos e outra para a variável de interesse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWdu3FT28xxJ"
   },
   "outputs": [],
   "source": [
    "X=df['content'].to_list()\n",
    "y=np.array(df[['Business', 'SciTech', 'Sports', 'World']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9TshYf48xxN"
   },
   "source": [
    "Checando um texto da lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "j8hNEXCF8xxO",
    "outputId": "213be4ce-db54-4db0-907e-d9b289adf140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\""
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ia-FtgF8xxU"
   },
   "source": [
    "Checando sua classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z1fMxmlc8xxV",
    "outputId": "c804d613-605b-4dda-94d8-7e62e2363cfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqWhMHCI8xxh"
   },
   "source": [
    "Definindo uma função para a limpeza dos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54-aI9788xxl"
   },
   "outputs": [],
   "source": [
    "def clean(resulta):   \n",
    "    result = copy.deepcopy(resulta)\n",
    "    \n",
    "    result=result.lower()\n",
    "    result=result.replace(\",\", \"\")\n",
    "    \n",
    "    result=re.sub(r\"\\@\\w+\", ' citation### ', result) #subtituindo @* por 'citation###'\n",
    "    result=re.sub(r\"http\\S+\", ' weblink### ', result) #subtituindo urls por 'weblink###'\n",
    "    result=re.sub('\\d', ' ### ', result) #subtituindo números por '###'\n",
    "    \n",
    "    result=re.sub('([.,!?()])', r' \\1 ', result)  #colocando espaço entre pontuações de palavras\n",
    "    result=re.sub('\\s{2,}', ' ', result)\n",
    "    \n",
    "    result=result.replace(\"<br />\",\"\")\n",
    "    result=result.replace(\"\\n\", \" \")\n",
    "    result=result.replace(\"/\", \"\")\n",
    "    result=result.replace(\"|\", \"\")\n",
    "    result=result.replace(\"+\", \"\")\n",
    "    \n",
    "    #result=result.replace(\".\", \"\") vamos MANTER \n",
    "    #result=result.replace(\":\", \"\") vamos MANTER \n",
    "    #result=result.replace(\";\", \"\") vamos MANTER \n",
    "    #result=result.replace(\"!\", \"\") vamos MANTER \n",
    "    #result=result.replace(\"?\", \"\") vamos MANTER \n",
    "    \n",
    "    result=result.replace(\">\", \"\")\n",
    "    result=result.replace(\"=\", \"\")\n",
    "    result=result.replace(\"§\", \"\")\n",
    "    result=result.replace(\" - \", \" \")\n",
    "    result=result.replace(\" _ \", \" \")\n",
    "    result=result.replace(\"&\", \"\")\n",
    "    result=result.replace(\"*\", \"\")\n",
    "    #result=result.replace(\"(\", \"\") vamos MANTER \n",
    "    #result=result.replace(\")\", \"\") vamos MANTER \n",
    "    result=result.replace(\"ª\", \"\")\n",
    "    result=result.replace(\"º\", \"\")\n",
    "    result=result.replace(\"%\", \"\")\n",
    "    result=result.replace(\"[\", \"\")\n",
    "    result=result.replace(\"]\", \"\")\n",
    "    result=result.replace(\"{\", \"\")\n",
    "    result=result.replace(\"}\", \"\")\n",
    "    result=result.replace(\"'\", \"\")\n",
    "    result=result.replace('\"', \"\")\n",
    "    result=result.replace(\"“\", \"\")\n",
    "    result=result.replace(\"”\", \"\")\n",
    "    result=re.sub(' +', ' ', result)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kellGBn38xxw"
   },
   "source": [
    "Limpando cada um dos textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFJwVZe08xxx"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i]=clean(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYy3-swj8xx2"
   },
   "source": [
    "Checando o mesmo texto de forma limpa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "nd24J0Cf8xx3",
    "outputId": "cd406d7e-bf19-4db2-c267-fbae9ee44e6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unions representing workers at turner newall say they are disappointed after talks with stricken parent firm federal mogul . '"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2I1fIE1TF68"
   },
   "source": [
    "Abrindo conjunto de Stopwords do pacote NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TU9R_on8xx9"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OC5tOWvLUSTy"
   },
   "source": [
    "Definindo função para a tokenização dos textos. A função já faz o processo de stemming e utiliza a base de stopwords para fazer a filtragem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PbwIW6u8xyB"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer() # para o port. >>> RSLPStemmer() \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        if item not in stop_words: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_zJNzCQ8xyE"
   },
   "source": [
    "Vamos dividir nossa base em bases de treino teste e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPjv38bg8xyF"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bH_TY3a75z0k",
    "outputId": "c9827c72-0da1-4121-bd84-1aebbb1db5fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6080,)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xnt57-1F8xyI"
   },
   "source": [
    "Treinaremos o modelo TFIDF (https://en.wikipedia.org/wiki/Tf%E2%80%93idf) para a extração de atributos dos textos. Na implementação abaixo, você pode ver que escolhemos trabalhar com unigramas e bigramas e isso quer dizer que se um texto da nossa base de dados fosse \"Eu como bolo\" trabalharíamos com os seguintes termos: \"Eu\", \"como\", \"bolo\", \"Eu como\", \"como bolo\". A partir de uma análise da nossa base de treino, o algoritmo dá um escore chamado IDF (Inverse Document Frequency) a cada um dos termos, sendo que termos que aparecem em menos textos têm maior escore - esses escores dados aos termos são ponderadores de quão importantes os termos são. No processo de construção de atributos de cada um dos textos, para cada um dos termos presentes nos textos calculamos a frequência relativa dos termos dentro de cada um dos textos (Term Frequency) e multiplicamos pelo escores 'IDF' de cada um dos termos (aquele calculado anteriormente). Dessa maneira, para cada um dos termos dentro de um texto, teremos potenciais variáveis com valores diferentes de zero (se a palavra não aparece naquele texto, então é automaticamente nula). Como pode-se perceber, teremos um número gigantesco de variáveis e para controlá-lo vamos impor um teto (max_features) de k variáveis. O pacote automaticamente seleciona os k termos com maior frequência em todo o corpus da base de treino. Vamos treinar o modelo TFIDF na base de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xz2P7l5m8xyK"
   },
   "outputs": [],
   "source": [
    "#Treinando bag of N-grams (TF-IDF) para a extração\n",
    "n_feat=10000\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize,ngram_range=(1,2), max_features=n_feat) \n",
    "tfidf.fit(X_train)\n",
    "\n",
    "#Salvando modelo\n",
    "#pickle.dump(tfidf, open(path+\"tfidf.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnu8T0zzeEHx"
   },
   "source": [
    "Vamos transformar nossas bases de treino, validação e test, que estão em formato de textos, em dados estruturados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKcSqjfg8xyP"
   },
   "outputs": [],
   "source": [
    "#Aplicando a transformação aprendida\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_val = tfidf.transform(X_val)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWMzAPIceXBU"
   },
   "source": [
    "Afim de aumentar o poder preditivo dos nossos modelos, vamos normalizar (para tudo ficar entre 0 e 1) nossos atributos em nossas bases de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm7Ttkg-8xyT"
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPyxCTE68xyZ"
   },
   "source": [
    "Checando formato de cada um dos arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AFP_YAKH8xyc",
    "outputId": "8c39e1ce-1dc3-4c0c-dc16-7443053e2c89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6080, 10000), (760, 10000), (760, 10000))"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train), np.shape(X_test), np.shape(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LhelcOUUe2kj",
    "outputId": "73e511e1-4dbd-4a29-be38-e79e3ed9b3c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6080, 4), (760, 4), (760, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train), np.shape(y_test), np.shape(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxECp58C8xyY"
   },
   "source": [
    "## Treinando e testando modelos preditivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdvcU4W28xyl"
   },
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACpEytUNe9HA"
   },
   "source": [
    "Para utilizar o modelo de regressão logística com o pacote Scikit-Learn, temos que transformar as nossas variáveis respostas em uma só da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LhvQc1op8xyn"
   },
   "outputs": [],
   "source": [
    "y_train2=np.argmax(y_train,axis=1)\n",
    "y_val2=np.argmax(y_val,axis=1)\n",
    "y_test2=np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EjCuUWkNhIXn"
   },
   "source": [
    "Em 'y_train2', 'y_test2' e 'y_val2' temos o seguinte encoding 'Business' -->0, 'SciTech' -->1, 'Sports' -->2 e 'World' -->3. Abaixo treinaremos um modelo de regressão logística multinomial, avaliando sua acurácia na base de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "49KigLpe8xyt",
    "outputId": "a5bd9999-2510-4176-ba1d-47cd3c9990b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia na base de validação= 0.8671052631578947\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100).fit(X_train, y_train2)\n",
    "logregScore = model.score(X_val, y_val2)\n",
    "print(\"Acurácia na base de validação=\",logregScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ5e6Lili6m1"
   },
   "source": [
    "Como temos um grande número de atributos, utilizaremos a penalidade do tipo 'L1' para fazer a seleção dos atributos mais importantes para a predição. Tendo que encontrar um bom parâmetro 'C' (*hiperparameter tuning*), que é o inverso do parâmetro de regularização, vamos aplicar o método de validação com grid-search, comparando diferentes valores de 'C', o número de atributos selecionados e a acurácia do modelo com os atributos selecionados na etapa anterior (ver https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "uTe3IkbA8xy4",
    "outputId": "d9059d6c-8eb1-4e49-92d6-e1e0104860e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 minutos \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04 minutos \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17 minutos \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85 minutos \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.72 minutos \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#\n",
    "cs=[.01,.1,1,10,100]\n",
    "#\n",
    "summary=[]\n",
    "\n",
    "for c in cs:\n",
    "    \n",
    "    #seleção de atributos\n",
    "    logreg = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l1',C=c, max_iter=100).fit(X_train, y_train2)\n",
    "    select_features = SelectFromModel(logreg, prefit=True)\n",
    "    \n",
    "    X_train_sel=select_features.transform(X_train)\n",
    "    X_test_sel=select_features.transform(X_test)\n",
    "    X_val_sel=select_features.transform(X_val)\n",
    "\n",
    "    #fittando o modelo\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100).fit(X_train_sel, y_train2)\n",
    "    \n",
    "    #avaliando acurácia\n",
    "    logregScore = model.score(X_val_sel, y_val2)\n",
    "    \n",
    "    #resumo da validação\n",
    "    summary.append((c,np.shape(X_train_sel)[1],logregScore))\n",
    "    \n",
    "    print(round((time.time() - start_time)/60,2),\"minutos \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LIZaTyBi5m1"
   },
   "source": [
    "Mesmo tendo alguns problemas de convergência, os resultados foram interessantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "ic_gPFRZ_jat",
    "outputId": "fb974744-ef4b-4885-f1c6-2291147b19d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=    0.01 --- # Features=    1 --- Acurácia=0.33\n",
      "C=    0.10 --- # Features=  159 --- Acurácia=0.77\n",
      "C=    1.00 --- # Features= 1516 --- Acurácia=0.84\n",
      "C=   10.00 --- # Features= 6411 --- Acurácia=0.87\n",
      "C=  100.00 --- # Features= 9752 --- Acurácia=0.87\n"
     ]
    }
   ],
   "source": [
    "for i in summary:\n",
    "  print(\"C=%8.2f --- # Features=%5d --- Acurácia=%3.2f\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNtIYx938xy9"
   },
   "source": [
    "Vamos escolher C=10 para fazer a seleção de nossos atributos daqui para frente. Fazendo a seleção e treinando modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "yXyfwFmJ8xy_",
    "outputId": "6a9cc6d4-8a60-4037-b44f-f5f5feffcdf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Seleção\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='saga', penalty='l1',C=10).fit(X_train, y_train2)\n",
    "select_features = SelectFromModel(logreg, prefit=True)\n",
    "\n",
    "X_train_sel=select_features.transform(X_train)\n",
    "X_test_sel=select_features.transform(X_test)\n",
    "X_val_sel=select_features.transform(X_val)\n",
    "\n",
    "#Treinando\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(X_train_sel, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ITqaJPGotyH"
   },
   "source": [
    "Avaliando modelo na base de teste. Na matriz de confusão, nas linhas temos as classes previstas (0,1,2,3) enquanto nas colunas temos as verdadeiras classes  (0,1,2,3) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "9_E7DhCnJxi-",
    "outputId": "071d1ae3-6455-4636-ae26-708f8ea7ed65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia na base de teste=0.87 \n",
      "\n",
      "[[149  24   3   9]\n",
      " [ 19 161   2   6]\n",
      " [  4   2 192   7]\n",
      " [  7  11   6 158]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Acurácia na base de teste=%3.2f \\n\" % model.score(X_test_sel, y_test2))\n",
    "\n",
    "#\n",
    "\n",
    "y_pred = model.predict(X_test_sel)\n",
    "print(confusion_matrix(y_pred,y_test2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia de Workshop NLP2 Felipe.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
